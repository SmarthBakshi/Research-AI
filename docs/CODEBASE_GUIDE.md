# ğŸ“š ResearchAI Codebase & Workflow Guide

## ğŸ¯ Overview

ResearchAI is an end-to-end **Retrieval-Augmented Generation (RAG) system** designed to ingest, process, embed, and semantically search scientific papers from arXiv. This guide explains the entire codebase structure, workflow, and component interactions.

---

## ğŸ—ï¸ System Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INTERFACE                            â”‚
â”‚                   (Gradio UI - Port 7860)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      API LAYER                                   â”‚
â”‚                  (FastAPI - Port 8000)                           â”‚
â”‚            Endpoints: /healthz, /ask (planned)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                       â”‚
         â–¼                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RETRIEVAL LAYER   â”‚              â”‚   GENERATION LAYER       â”‚
â”‚   (OpenSearch)     â”‚              â”‚  (LLM via Ollama)        â”‚
â”‚  - Vector Search   â”‚              â”‚  - Prompt Construction   â”‚
â”‚  - BM25 Search     â”‚              â”‚  - Answer Generation     â”‚
â”‚  - Hybrid Search   â”‚              â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²                                       
         â”‚                                       
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PROCESSING PIPELINE                             â”‚
â”‚                  (Airflow DAGs)                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. Ingest    â”‚â†’ â”‚ 2. Process   â”‚â†’ â”‚ 3. Embed & Index     â”‚ â”‚
â”‚  â”‚    PDFs      â”‚  â”‚    PDFs      â”‚  â”‚                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚               â”‚                    â”‚
           â–¼               â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    MinIO     â”‚  â”‚  PostgreSQL  â”‚   â”‚   OpenSearch    â”‚
â”‚ (PDF Store)  â”‚  â”‚ (Metadata &  â”‚   â”‚ (Vector Index)  â”‚
â”‚              â”‚  â”‚  Chunks)     â”‚   â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Repository Structure

```
Research-AI/
â”œâ”€â”€ api/                          # API service code
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ core/                         # Core library code
â”‚   â””â”€â”€ src/researchai_core/
â”‚
â”œâ”€â”€ dags/                         # Airflow DAG definitions
â”‚   â”œâ”€â”€ ingest_arxiv.py          # DAG 1: Download PDFs from arXiv
â”‚   â”œâ”€â”€ process_pdfs.py          # DAG 2: Extract & chunk text
â”‚   â””â”€â”€ embed_and_index.py       # DAG 3: Generate embeddings & index
â”‚
â”œâ”€â”€ services/                     # Modular service components
â”‚   â”œâ”€â”€ chunkers/                # Text chunking strategies
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ heading_aware.py
â”‚   â”‚   â””â”€â”€ sliding_window.py
â”‚   â”‚
â”‚   â”œâ”€â”€ embedding/               # Embedding generation
â”‚   â”‚   â”œâ”€â”€ embedder.py         # Abstract base class
â”‚   â”‚   â””â”€â”€ huggingface_embedder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/              # Data ingestion from arXiv
â”‚   â”‚   â””â”€â”€ arxiv/
â”‚   â”‚       â”œâ”€â”€ client.py       # arXiv API client
â”‚   â”‚       â”œâ”€â”€ config.py
â”‚   â”‚       â”œâ”€â”€ minio_utils.py  # MinIO helper functions
â”‚   â”‚       â”œâ”€â”€ runner.py       # Ingestion orchestration
â”‚   â”‚       â””â”€â”€ writer.py       # Write to MinIO
â”‚   â”‚
â”‚   â”œâ”€â”€ processing/             # PDF processing pipeline
â”‚   â”‚   â”œâ”€â”€ extractors/        # PDF text extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”œâ”€â”€ hybrid_extractor.py  # Smart PDF + OCR
â”‚   â”‚   â”‚   â””â”€â”€ pdfminer_extractor.py
â”‚   â”‚   â”œâ”€â”€ chunking/          # Text chunking
â”‚   â”‚   â”‚   â””â”€â”€ chunker.py
â”‚   â”‚   â”œâ”€â”€ db_write/          # Database operations
â”‚   â”‚   â”‚   â””â”€â”€ db_writer.py
â”‚   â”‚   â”œâ”€â”€ normalization/     # Text cleaning
â”‚   â”‚   â”‚   â””â”€â”€ text_normaliser.py
â”‚   â”‚   â””â”€â”€ ocr/               # OCR fallback
â”‚   â”‚       â””â”€â”€ tesseract_ocr.py
â”‚   â”‚
â”‚   â””â”€â”€ search/                 # Search & retrieval
â”‚       â”œâ”€â”€ opensearch_store.py # OpenSearch client
â”‚       â””â”€â”€ vector_store.py
â”‚
â”œâ”€â”€ docker/                      # Docker configurations
â”‚   â”œâ”€â”€ api/                    # API service Dockerfile
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ app/main.py
â”‚   â””â”€â”€ ui/                     # UI service Dockerfile
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ app.py
â”‚
â”œâ”€â”€ infra/                       # Infrastructure configs
â”‚   â””â”€â”€ airflow/
â”‚       â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â””â”€â”€ researchai/
â”‚       â”œâ”€â”€ embedder_test.py
â”‚       â”œâ”€â”€ ingest_metadata.py
â”‚       â””â”€â”€ ingest_pdf.py
â”‚
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ processing/
â”‚   â””â”€â”€ unit/
â”‚
â”œâ”€â”€ data/                        # Data directory
â”‚   â””â”€â”€ arxiv/
â”‚       â””â”€â”€ metadata.json       # arXiv paper metadata
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ architecture.gif
â”‚   â””â”€â”€ CODEBASE_GUIDE.md       # This file
â”‚
â”œâ”€â”€ docker-compose.yml           # Service orchestration
â”œâ”€â”€ pyproject.toml              # Python dependencies
â”œâ”€â”€ Makefile                    # Common commands
â””â”€â”€ README.md                   # Project README
```

---

## ğŸ”„ Complete Data Flow Workflow

### Phase 1: Ingestion (DAG: `ingest_arxiv_pdf`)

**File:** `dags/ingest_arxiv.py`

**Purpose:** Download PDFs from arXiv and store them in MinIO

**Flow:**
```
1. read_metadata()
   â”œâ”€ Reads: /opt/researchai/data/arxiv/metadata.json
   â””â”€ Returns: List of paper metadata dicts

2. ingest(metadata)
   â”œâ”€ For each paper in metadata:
   â”‚  â”œâ”€ Extract arXiv ID
   â”‚  â”œâ”€ Construct PDF URL: https://arxiv.org/pdf/{arxiv_id}.pdf
   â”‚  â”œâ”€ Download PDF content
   â”‚  â”œâ”€ Calculate SHA256 hash
   â”‚  â”œâ”€ Upload to MinIO: raw/pdfs/{hash}.pdf
   â”‚  â””â”€ Skip if already exists
   â””â”€ Returns: List of ingestion results
```

**Key Components:**
- `services/ingestion/arxiv/runner.py`: Orchestrates ingestion
- `services/ingestion/arxiv/writer.py`: MinIOWriter class
  - Uses boto3 to interact with MinIO (S3-compatible API)
  - Implements content-addressable storage (filename = hash of content)
  - Idempotent: checks if PDF exists before uploading

**Storage Location:**
- MinIO bucket: `researchai`
- Path pattern: `raw/pdfs/{sha256_hash}.pdf`

---

### Phase 2: Processing (DAG: `process_pdfs`)

**File:** `dags/process_pdfs.py`

**Purpose:** Extract text from PDFs, normalize, chunk, and store in PostgreSQL

**Flow:**
```
1. list_pdf_keys()
   â”œâ”€ Connects to MinIO
   â”œâ”€ Lists all files in bucket 'researchai'
   â””â”€ Returns: List of PDF keys

2. process_each_pdf(key)  [Dynamic task mapping - runs in parallel]
   â”‚
   â”œâ”€ Step 1: Download PDF from MinIO
   â”‚  â””â”€ services/ingestion/arxiv/minio_utils.download_file()
   â”‚
   â”œâ”€ Step 2: Extract Text (Hybrid approach)
   â”‚  â”œâ”€ Primary: HybridPdfExtractor.extract()
   â”‚  â”‚  â”œâ”€ Uses PdfMinerExtractor first
   â”‚  â”‚  â”‚  â””â”€ services/processing/extractors/pdfminer_extractor.py
   â”‚  â”‚  â”œâ”€ Detects "sparse" pages (< 20 chars/page)
   â”‚  â”‚  â””â”€ Falls back to OCR for sparse pages
   â”‚  â”‚     â””â”€ services/processing/ocr/tesseract_ocr.py
   â”‚  â””â”€ Fallback: Full OCR if primary fails
   â”‚
   â”œâ”€ Step 3: Normalize Text
   â”‚  â””â”€ TextNormalizer.clean()
   â”‚     â””â”€ services/processing/normalization/text_normaliser.py
   â”‚     â”œâ”€ Removes LaTeX commands
   â”‚     â”œâ”€ Fixes spacing issues
   â”‚     â”œâ”€ Removes special characters
   â”‚     â””â”€ Normalizes whitespace
   â”‚
   â”œâ”€ Step 4: Chunk Text
   â”‚  â””â”€ Chunker.chunk()
   â”‚     â””â”€ services/processing/chunking/chunker.py
   â”‚     â”œâ”€ Sliding window approach
   â”‚     â”œâ”€ Default: 300 words per chunk
   â”‚     â”œâ”€ Overlap: 50 words
   â”‚     â””â”€ Returns: List of {source_file, chunk_index, chunk_text}
   â”‚
   â”œâ”€ Step 5: Store in PostgreSQL
   â”‚  â””â”€ write_chunks_to_db()
   â”‚     â””â”€ services/processing/db_write/db_writer.py
   â”‚     â”œâ”€ Table: chunks
   â”‚     â”œâ”€ Columns: id, source_file, chunk_index, chunk_text, created_at
   â”‚     â””â”€ Uses SQLAlchemy ORM
   â”‚
   â””â”€ Step 6: Cleanup
      â””â”€ Delete local PDF file
```

**Key Components:**

1. **HybridPdfExtractor** (`services/processing/extractors/hybrid_extractor.py`)
   - Intelligent extraction: tries PDF text first, falls back to OCR
   - Returns ExtractResult object with text, pages, and error flags

2. **TextNormalizer** (`services/processing/normalization/text_normaliser.py`)
   - Cleans and standardizes extracted text
   - Removes LaTeX artifacts, fixes spacing, normalizes unicode

3. **Chunker** (`services/processing/chunking/chunker.py`)
   - Sliding window chunking strategy
   - Configurable chunk size and overlap
   - Preserves context across chunks

4. **Database Writer** (`services/processing/db_write/db_writer.py`)
   - SQLAlchemy-based PostgreSQL client
   - Auto-creates tables if they don't exist
   - Stores chunks with metadata

**Error Handling:**
- Failed PDFs logged to: `/opt/airflow/logs/quarantine_keys.txt`
- Includes timestamp, key, and error message
- Task execution timeout: 10 minutes

**Parallelization:**
- Uses Airflow dynamic task mapping
- `max_active_tasks=4`: Process 4 PDFs concurrently
- Each PDF is a separate task instance

---

### Phase 3: Embedding & Indexing (DAG: `embed_and_index`)

**File:** `dags/embed_and_index.py`

**Purpose:** Generate embeddings for chunks and index them in OpenSearch

**Flow:**
```
1. load_unindexed_chunks()
   â”œâ”€ Query PostgreSQL: SELECT chunks WHERE indexed_at IS NULL
   â”œâ”€ LIMIT 100 (batch size)
   â””â”€ Returns: List of {chunk_id, chunk_text, source_file, chunk_index}

2. embed_chunks(chunks)
   â”œâ”€ Initialize: HuggingFaceEmbedder("intfloat/e5-base-v2")
   â”‚  â””â”€ services/embedding/huggingface_embedder.py
   â”‚  â””â”€ Model: 768-dimensional embeddings
   â”œâ”€ Extract text from all chunks
   â”œâ”€ Generate embeddings (batch processing)
   â””â”€ Returns: chunks + embeddings

3. upsert_to_opensearch(chunks)
   â”œâ”€ Initialize: OpenSearchStore
   â”‚  â””â”€ services/search/opensearch_store.py
   â”œâ”€ Bulk upload to OpenSearch
   â”‚  â”œâ”€ Index: 'chunks'
   â”‚  â”œâ”€ Document ID: {source_file}_{chunk_index}
   â”‚  â””â”€ Fields: chunk_text, embedding, chunk_index, source_file
   â””â”€ Returns: List of chunk_ids

4. mark_indexed(chunk_ids)
   â”œâ”€ UPDATE chunks SET indexed_at = NOW()
   â””â”€ WHERE chunk_id IN (chunk_ids)
```

**Key Components:**

1. **HuggingFaceEmbedder** (`services/embedding/huggingface_embedder.py`)
   - Uses sentence-transformers library
   - Default model: `intfloat/e5-base-v2`
   - Generates 768-dimensional dense vectors
   - Batch processing for efficiency

2. **OpenSearchStore** (`services/search/opensearch_store.py`)
   - Vector database client
   - Supports three search modes:
     - **Dense Search**: Pure kNN vector similarity
     - **BM25 Search**: Traditional keyword search
     - **Hybrid Search**: Combined vector + BM25
   - Index configuration:
     - `knn_vector` field with HNSW algorithm
     - `cosinesimil` space type
     - `nmslib` engine

**Database Schema Updates:**
```sql
-- chunks table includes:
indexed_at TIMESTAMP  -- NULL = not indexed, NOT NULL = indexed
```

**Batch Processing:**
- Processes 100 chunks per DAG run
- Prevents memory overflow
- Allows incremental indexing

---

## ğŸ¨ Component Deep Dive

### 1. PDF Text Extraction

**Architecture:**
```
HybridPdfExtractor (hybrid_extractor.py)
    â”‚
    â”œâ”€â–º PdfMinerExtractor (pdfminer_extractor.py)
    â”‚   â”œâ”€ Library: pdfminer.six
    â”‚   â”œâ”€ Extracts: Structured PDF text
    â”‚   â”œâ”€ Works on: Text-based PDFs
    â”‚   â””â”€ Detects: Sparse pages (< min_chars_per_page)
    â”‚
    â””â”€â–º TesseractOCR (tesseract_ocr.py)
        â”œâ”€ Library: pytesseract + pdf2image
        â”œâ”€ Extracts: Text from images
        â”œâ”€ Works on: Scanned PDFs, images
        â””â”€ Returns: Text + confidence score
```

**Strategy:**
1. Try PdfMiner first (fast, works for 90% of papers)
2. Identify pages with < 20 characters (likely scanned/image)
3. Run OCR only on sparse pages (hybrid approach)
4. If PdfMiner returns nothing, run full OCR

**Return Type:**
```python
@dataclass
class ExtractResult:
    text: str                  # Full extracted text
    pages: List[PageText]      # Per-page details
    errors: List[str]          # Error messages
    used_ocr: bool            # Whether OCR was used
```

---

### 2. Text Chunking Strategies

**Current Implementation:** Sliding Window

**Code:** `services/processing/chunking/chunker.py`

```python
class Chunker:
    def __init__(self, chunk_size=300, overlap=50):
        self.chunk_size = 300    # words per chunk
        self.overlap = 50        # overlapping words
```

**Algorithm:**
```
Input: "word1 word2 word3 ... wordN"

Chunk 1: words[0:300]
Chunk 2: words[250:550]    # 50-word overlap with chunk 1
Chunk 3: words[500:800]
...
```

**Why Overlap?**
- Preserves context across chunk boundaries
- Improves retrieval accuracy
- Prevents splitting of related concepts

**Alternative Chunkers** (in codebase but not used):
- `heading_aware.py`: Chunks based on document structure
- `sliding_window.py`: Pure sliding window

---

### 3. Embedding Generation

**Model:** `intfloat/e5-base-v2`

**Characteristics:**
- Type: Dense vector embeddings
- Dimensions: 768
- Library: HuggingFace sentence-transformers
- Training: Contrastive learning on diverse text

**Usage Pattern:**
```python
embedder = HuggingFaceEmbedder("intfloat/e5-base-v2")
embeddings = embedder.embed(["text1", "text2", ...])
# Returns: [[0.1, 0.2, ...], [0.3, 0.4, ...]]
```

**Performance Considerations:**
- Batch processing: More efficient than one-by-one
- GPU acceleration: Not currently configured (CPU only)
- Model loading: Cached after first use

---

### 4. Search & Retrieval

**OpenSearch Index Schema:**
```json
{
  "mappings": {
    "properties": {
      "embedding": {
        "type": "knn_vector",
        "dimension": 768,
        "method": {
          "name": "hnsw",
          "space_type": "cosinesimil",
          "engine": "nmslib"
        }
      },
      "chunk_text": {"type": "text"},
      "chunk_index": {"type": "integer"},
      "source_file": {"type": "keyword"}
    }
  }
}
```

**Search Methods:**

1. **Dense kNN Search:**
   ```python
   store.search_dense(query_vector, k=5)
   ```
   - Pure semantic similarity
   - Uses cosine similarity
   - Fast approximate nearest neighbors (HNSW)

2. **Hybrid Search:**
   ```python
   store.search_hybrid(query_vector, query_text, k=5)
   ```
   - Combines BM25 keyword matching + vector similarity
   - More robust than either alone
   - Better handles specific terms + semantic meaning

---

## ğŸ³ Docker Services

### Service Dependency Graph

```
postgres
    â”œâ”€â–º airflow-init
    â”‚       â””â”€â–º airflow
    â”œâ”€â–º api
    â””â”€â–º (app database)

opensearch
    â””â”€â–º api

minio
    â”œâ”€â–º api
    â””â”€â–º airflow (for PDF storage)

api
    â””â”€â–º ui
```

### Service Details

| Service | Image | Port | Purpose |
|---------|-------|------|---------|
| **postgres** | postgres:15 | 5435â†’5432 | Two databases: `researchai_app` + `researchai_airflow` |
| **opensearch** | opensearchproject/opensearch:2.11.0 | 9200 | Vector search engine |
| **minio** | minio/minio | 9000 (API), 9001 (Console) | S3-compatible object storage |
| **airflow** | researchai-airflow:2.9.0 | 8080 | DAG orchestration |
| **api** | custom | 8000 | FastAPI backend |
| **ui** | custom | 7860 | Gradio interface |

### Volume Mounts (Airflow Container)

```yaml
volumes:
  - ./data:/opt/airflow/data              # Metadata files
  - ./dags:/opt/airflow/dags              # DAG definitions
  - ./services:/opt/researchai/services   # Service modules
  - ./scripts:/opt/researchai/scripts     # Utility scripts
```

**Why?**
- Changes to DAGs/services reflected immediately
- No rebuild required for code changes
- Facilitates development workflow

---

## ğŸš€ Development Workflow

### Setting Up the Environment

1. **Clone the repository:**
   ```bash
   git clone https://github.com/SmarthBakshi/Research-AI
   cd Research-AI
   ```

2. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

3. **Start services:**
   ```bash
   make build-up
   # or
   docker compose up -d --build
   ```

4. **Access interfaces:**
   - Airflow UI: http://localhost:8080
   - MinIO Console: http://localhost:9001
   - API Docs: http://localhost:8000/docs
   - Gradio UI: http://localhost:7860

### Running the Pipeline

**Step 1: Prepare Metadata**
```bash
# Place your arXiv metadata in:
data/arxiv/metadata.json

# Format:
[
  {
    "id": "2401.12345",
    "title": "Paper Title",
    "authors": [...],
    ...
  }
]
```

**Step 2: Trigger Ingestion DAG**
1. Go to Airflow UI (http://localhost:8080)
2. Login: admin/admin
3. Enable DAG: `ingest_arxiv_pdf`
4. Trigger manually
5. Monitor progress in the DAG graph view

**Step 3: Trigger Processing DAG**
1. Enable DAG: `process_pdfs`
2. Trigger manually
3. Watch parallel task execution
4. Check logs if any task fails

**Step 4: Trigger Embedding DAG**
1. Enable DAG: `embed_and_index`
2. Trigger manually
3. May need to run multiple times (100 chunks per run)

### Common Development Tasks

**Run tests:**
```bash
make test
# or
pytest -v tests/
```

**Lint code:**
```bash
make lint
# or
ruff check .
```

**Format code:**
```bash
make format
# or
black .
```

**Rebuild specific service:**
```bash
make rebuild-airflow
make rebuild-api
make rebuild-ui
```

**Check service logs:**
```bash
docker compose logs -f airflow
docker compose logs -f api
```

**Connect to PostgreSQL:**
```bash
docker exec -it researchai-postgres psql -U researchai -d researchai_app
```

**Browse MinIO:**
```bash
# Web UI: http://localhost:9001
# Login: minio / minio123
```

---

## ğŸ§ª Testing Strategy

### Test Organization

```
tests/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ test_arxiv_client.py        # arXiv API client tests
â”‚   â””â”€â”€ test_ingest_arxiv_dag.py    # DAG logic tests
â”‚
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ test_pdf_extractor.py       # PDF extraction tests
â”‚   â”œâ”€â”€ test_ocr_wrapper.py         # OCR functionality tests
â”‚   â”œâ”€â”€ test_chunkers.py            # Chunking strategy tests
â”‚   â”œâ”€â”€ test_text_normaliser.py     # Text normalization tests
â”‚   â””â”€â”€ test_db_writer.py           # Database writing tests
â”‚
â””â”€â”€ unit/
    â””â”€â”€ test_arxiv_client.py        # Unit tests
```

### Running Tests

```bash
# All tests
pytest -v tests/

# Specific module
pytest tests/processing/test_chunkers.py

# With coverage
pytest --cov=services tests/
```

---

## ğŸ”§ Configuration

### Environment Variables

**PostgreSQL:**
```bash
POSTGRES_USER=researchai
POSTGRES_PASSWORD=researchai
APP_DB=researchai_app              # Application database
AIRFLOW_DB=researchai_airflow      # Airflow metadata database
```

**OpenSearch:**
```bash
OPENSEARCH_PORT=9200
OPENSEARCH_HOST=opensearch
DISABLE_SECURITY_PLUGIN=true       # For development only
```

**MinIO:**
```bash
MINIO_ROOT_USER=minio
MINIO_ROOT_PASSWORD=minio123
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001
MINIO_BUCKET=researchai
```

**Airflow:**
```bash
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PWD=admin
AIRFLOW_ADMIN_EMAIL=admin@example.com
```

### Changing Configuration

1. **Chunking parameters:**
   Edit `services/processing/chunking/chunker.py`:
   ```python
   def __init__(self, chunk_size=300, overlap=50):
   ```

2. **Embedding model:**
   Edit `dags/embed_and_index.py`:
   ```python
   embedder = HuggingFaceEmbedder("intfloat/e5-base-v2")
   # Change to your preferred model
   ```

3. **Batch size:**
   Edit `dags/embed_and_index.py`:
   ```sql
   LIMIT 100;  -- Change batch size
   ```

4. **Parallel tasks:**
   Edit `dags/process_pdfs.py`:
   ```python
   max_active_tasks=4  # Increase for more parallelism
   ```

---

## ğŸ¯ Current Status & Roadmap

### âœ… Implemented (M1-M4)

- [x] Docker infrastructure with all services
- [x] MinIO object storage for PDFs
- [x] PostgreSQL for metadata and chunks
- [x] Airflow DAGs for pipeline orchestration
- [x] PDF ingestion from arXiv
- [x] Hybrid text extraction (PDF + OCR)
- [x] Text normalization and chunking
- [x] HuggingFace embedding generation
- [x] OpenSearch indexing with kNN support
- [x] Basic API skeleton
- [x] Basic UI skeleton

### ğŸš§ In Progress (M5)

- [ ] Implement `/ask` endpoint in FastAPI
- [ ] Integrate retrieval with LLM (Ollama)
- [ ] Prompt engineering for RAG
- [ ] Response generation pipeline
- [ ] Citation tracking

### ğŸ“‹ Planned (M6)

- [ ] Langfuse integration for observability
- [ ] Latency monitoring (p95 < 1.5s target)
- [ ] User feedback collection
- [ ] Quality metrics (hit@k, token usage)
- [ ] Comprehensive test coverage
- [ ] CI/CD pipeline
- [ ] Production deployment guide

---

## ğŸ” Key Design Decisions

### 1. Why Hybrid PDF Extraction?

**Problem:** Scientific PDFs come in two forms:
- Text-based PDFs (LaTeX-generated): Easy to extract
- Scanned PDFs (old papers): Require OCR

**Solution:** HybridPdfExtractor
- Try fast text extraction first
- Detect problematic pages
- Apply OCR selectively
- **Result:** Best of both worlds - speed + accuracy

### 2. Why Content-Addressable Storage?

**Problem:** Same PDF might be ingested multiple times

**Solution:** Hash-based filenames
```python
filename = f"{sha256(pdf_content)}.pdf"
```
- Same content = same filename
- Automatic deduplication
- No database lookup needed

### 3. Why Sliding Window Chunking?

**Problem:** Fixed-size chunks might split related text

**Solution:** Overlapping chunks
- Context preserved across boundaries
- Better retrieval accuracy
- Slight storage overhead (acceptable trade-off)

### 4. Why OpenSearch over Alternatives?

**Comparison:**
| Feature | OpenSearch | Pinecone | Weaviate |
|---------|-----------|----------|----------|
| Open Source | âœ… | âŒ | âœ… |
| Hybrid Search | âœ… | âŒ | âœ… |
| Self-Hosted | âœ… | âŒ | âœ… |
| BM25 + Vector | âœ… | âŒ | âœ… |

**Decision:** OpenSearch for hybrid search + self-hosting

### 5. Why Airflow for Orchestration?

**Alternatives:** Prefect, Dagster, Temporal

**Airflow Advantages:**
- Mature ecosystem
- Strong DAG visualization
- Built-in retry/failure handling
- Dynamic task mapping
- Large community

---

## ğŸ› Troubleshooting

### Common Issues

**1. Airflow DAG not appearing:**
```bash
# Check DAG syntax
docker exec -it researchai-airflow airflow dags list

# View DAG errors
docker compose logs airflow | grep ERROR
```

**2. MinIO connection failed:**
```bash
# Check MinIO is running
curl http://localhost:9000/minio/health/ready

# Verify credentials in .env
echo $MINIO_ROOT_USER
echo $MINIO_ROOT_PASSWORD
```

**3. PostgreSQL connection refused:**
```bash
# Check postgres is healthy
docker compose ps postgres

# Test connection
docker exec -it researchai-postgres psql -U researchai -d researchai_app -c "SELECT 1;"
```

**4. OpenSearch not starting (Apple Silicon):**
```yaml
# In docker-compose.yml, uncomment:
opensearch:
  platform: linux/amd64
```

**5. Out of memory during embedding:**
```python
# In embed_and_index.py, reduce batch size:
LIMIT 50;  # Instead of 100
```

### Debug Mode

**Enable Airflow debug logs:**
```yaml
# In docker-compose.yml:
environment:
  AIRFLOW__LOGGING__LOGGING_LEVEL: DEBUG
```

**Check DAG execution logs:**
```bash
# Via UI: Airflow â†’ DAGs â†’ [DAG name] â†’ Graph â†’ [Task] â†’ Logs

# Via CLI:
docker exec -it researchai-airflow airflow tasks test ingest_arxiv_pdf read_metadata 2024-01-01
```

---

## ğŸ“š Additional Resources

### Key Technologies

- **Airflow:** https://airflow.apache.org/docs/
- **OpenSearch:** https://opensearch.org/docs/latest/
- **MinIO:** https://min.io/docs/minio/linux/index.html
- **HuggingFace Embeddings:** https://huggingface.co/docs/transformers/
- **FastAPI:** https://fastapi.tiangolo.com/

### Related Papers

- "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" (Lewis et al., 2020)
- "Dense Passage Retrieval for Open-Domain Question Answering" (Karpukhin et al., 2020)
- "Text and Code Embeddings by Contrastive Pre-Training" (Wang et al., 2022)

### Community

- GitHub Issues: https://github.com/SmarthBakshi/Research-AI/issues
- Discussions: https://github.com/SmarthBakshi/Research-AI/discussions

---

## ğŸ¤ Contributing

### Code Style

- **Formatter:** Black (line length 100)
- **Linter:** Ruff
- **Type Hints:** Encouraged but not required

### Pull Request Process

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `make test`
5. Run linter: `make lint`
6. Format code: `make format`
7. Submit PR with description

### Commit Message Convention

```
<type>(<scope>): <subject>

Examples:
feat(dags): add metadata validation step
fix(extractor): handle empty PDF pages
docs(readme): update installation steps
test(chunker): add overlap boundary tests
```

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ‘¥ Credits

**Author:** Smarth Bakshi (bakshismarth.20@gmail.com)

**Contributors:** [See GitHub contributors page]

---

**Last Updated:** 2025-10-28
