# ğŸš€ ResearchAI: RAG System for Scientific Papers

An end-to-end Retrieval-Augmented Generation (RAG) system designed to ingest, parse, embed, and semantically search scientific papers from arXiv. Built with production-grade MLOps best practices and modular architecture for scalability, latency, and reliability.

> ğŸš§ **This project is currently under active development.**  

---

## ğŸ§  Project Goals

- Ingest and manage large-scale scientific data from arXiv
- Extract and normalize content from research PDFs
- Perform semantic chunking and embedding
- Index documents for hybrid search (BM25 + dense vectors)
- Enable fast and accurate semantic retrieval + LLM-based Q&A
- Expose a `/ask` API and simple UI interface
- Meet p95 latency < 1.5 seconds per query

---

## ğŸ“š Documentation

- **[ğŸ“– Complete Codebase Guide](docs/CODEBASE_GUIDE.md)** - Deep dive into the entire codebase, architecture, and design decisions
- **[âš¡ Workflow Quick Start](docs/WORKFLOW_QUICKSTART.md)** - Step-by-step guide to running the pipeline
- **[ğŸ—ï¸ Architecture Overview](docs/ARCHITECTURE.md)** - System architecture, component interactions, and data flows

---
## â‡¢ Data Flow 

![Diagram GIF](docs/architecture.gif) 


### ğŸ”„ Primary Data Flow (Papers â†’ Answers)

#### ğŸ“š Data source
- arXiv API (papers + metadata)

#### ğŸ“¥ Ingestion & Orchestration Layer
- Airflow DAG pulls metadata + PDFs


#### ğŸª£ Object Storage Layer (MinIO)
- Raw JSON metadata & PDFs saved


#### ğŸ§ª Processing Layer
- PDF â†’ text â†’ normalized â†’ chunked â†’ embeddings generated

#### ğŸ§¾ Structured Data Storage Layer

- ğŸ§  Embeddings â†’ stored in OpenSearch
- ğŸ“„ Metadata â†’ stored in Postgres

#### ğŸ” Retrieval Layer

- Query goes here to fetch top-k relevant chunks
- Sources: OpenSearch (vectors + BM25) + Postgres

#### ğŸ’¬ LLM Generation Layer

- Prompt is built using retrieved chunks
- LLM (via Ollama/OpenRouter/etc.) generates grounded answer

#### ğŸŒ API Layer (FastAPI)

- Manages request/response
- Orchestrates retrieval + generation

#### ğŸ‘¤ Client Interface Layer (Gradio)

- User enters question

- Receives answer + citations

#### ğŸ“Š Observability Layer (Langfuse)
It monitors the entire system, especially:
- Latency (e.g. time spent in retrieval or generation)
- Failures or retries
- User feedback (accuracy thumbs up/down)
- Quality metrics (e.g. token usage, hit@k, costs)
- Collects logs and traces 



### ğŸ” Reverse Flow (User Query â†’ Answer)
- Client Interface â†’ API Layer â†’ Retrieval Layer â†’ LLM Gen â†’ API â†’ Client Interface


---

## ğŸ§ª Key Features

- âœ… **Airflow DAGs** for idempotent ingestion & processing
- âœ… **MinIO Object Store** for metadata and PDFs
- âœ… **Custom PDF Extractor** with smart chunking
- âœ… **Embedding pipeline** with HuggingFace transformers
- âœ… **Hybrid Search Engine** using OpenSearch (BM25 + dense)
- âœ… **FastAPI** backend and `/ask` query endpoint
- âœ… **Modular UI (Gradio)** for demo and testing
- âœ… **Dockerized**, CI-ready, extensible

<!-- --- -->

<!-- ## ğŸ“Š Performance Snapshot

| Metric             | Value            |
|--------------------|------------------|
| Ingestion Rate     | ~100 PDFs/min    |
| p95 Query Latency  | 1.2s             |
| Retrieval Quality  | Recall@10 = 0.84 |
| Cost per Query     | $0.004 (est.)    | -->

---

## ğŸ§° Tech Stack

| Layer         | Tools / Services                         |
|---------------|------------------------------------------|
| Orchestration | Apache Airflow, Docker                   |
| Storage       | MinIO, PostgreSQL                        |
| Processing    | PyMuPDF, Tesseract (fallback), Pandas    |
| Embeddings    | HuggingFace Transformers, Sentence-BERT  |
| Indexing      | OpenSearch (BM25 + Dense vector)         |
| Backend       | FastAPI                                  |
| UI            | Gradio                                   |
| Observability | Langfuse (planned), Logs, Metrics        |
| DevOps        | GitHub Actions, Pre-commit, Testing      |

<!-- --- -->

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/research-ai
cd research-ai
```

### Configure Environment

```bash
cp .env.example .env
# Fill in your MinIO credentials, ports, etc.
```

### Start Services

```bash
docker compose up -d --build
```

---
### Access interfaces:

ğŸŒ Airflow UI: http://localhost:8080

ğŸ§  OpenSearch UI: http://localhost:5601
 (if enabled)

ğŸ“‚ MinIO Console: http://localhost:9001

ğŸ§ª FastAPI Docs: http://localhost:8000/docs

---

## ğŸ§ª Development Roadmap

 M1: Infra foundation (Docker, services, health checks)

 M2: Ingestion DAG + PDF storage

 M3: Text extraction & chunking

 M4: Embedding & Indexing

 M5: Search API + LLM response generation

 M6: Observability, tests, documentation