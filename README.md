# ðŸš€ ResearchAI: RAG System for Scientific Papers

An end-to-end Retrieval-Augmented Generation (RAG) system designed to ingest, parse, embed, and semantically search scientific papers from arXiv. Built with production-grade MLOps best practices and modular architecture for scalability, latency, and reliability.

> ðŸš§ **This project is currently under active development.**  

---

## ðŸ§  Project Goals

- Ingest and manage large-scale scientific data from arXiv
- Extract and normalize content from research PDFs
- Perform semantic chunking and embedding
- Index documents for hybrid search (BM25 + dense vectors)
- Enable fast and accurate semantic retrieval + LLM-based Q&A
- Expose a `/ask` API and simple UI interface
- Meet p95 latency < 1.5 seconds per query

---
## â‡¢ Data Flow 

![Diagram GIF](docs/architecture.gif)


### ðŸ”„ Primary Data Flow (Papers â†’ Answers)

#### ðŸ“š Data source
- arXiv API (papers + metadata)

#### ðŸ“¥ Ingestion & Orchestration Layer
- Airflow DAG pulls metadata + PDFs


#### ðŸª£ Object Storage Layer (MinIO)
- Raw JSON metadata & PDFs saved


#### ðŸ§ª Processing Layer
- PDF â†’ text â†’ normalized â†’ chunked â†’ embeddings generated

#### ðŸ§¾ Structured Data Storage Layer

- ðŸ§  Embeddings â†’ stored in OpenSearch
- ðŸ“„ Metadata â†’ stored in Postgres

#### ðŸ” Retrieval Layer

- Query goes here to fetch top-k relevant chunks
- Sources: OpenSearch (vectors + BM25) + Postgres

#### ðŸ’¬ LLM Generation Layer

- Prompt is built using retrieved chunks
- LLM (via Ollama/OpenRouter/etc.) generates grounded answer

#### ðŸŒ API Layer (FastAPI)

- Manages request/response
- Orchestrates retrieval + generation

#### ðŸ‘¤ Client Interface Layer (Gradio)

- User enters question

- Receives answer + citations

#### ðŸ“Š Observability Layer (Langfuse)
It monitors the entire system, especially:
- Latency (e.g. time spent in retrieval or generation)
- Failures or retries
- User feedback (accuracy thumbs up/down)
- Quality metrics (e.g. token usage, hit@k, costs)
- Collects logs and traces 



### ðŸ” Reverse Flow (User Query â†’ Answer)
- Client Interface â†’ API Layer â†’ Retrieval Layer â†’ LLM Gen â†’ API â†’ Client Interface


---

## ðŸ§ª Key Features

- âœ… **Airflow DAGs** for idempotent ingestion & processing
- âœ… **MinIO Object Store** for metadata and PDFs
- âœ… **Custom PDF Extractor** with smart chunking
- âœ… **Embedding pipeline** with HuggingFace transformers
- âœ… **Hybrid Search Engine** using OpenSearch (BM25 + dense)
- âœ… **FastAPI** backend and `/ask` query endpoint
- âœ… **Modular UI (Gradio)** for demo and testing
- âœ… **Dockerized**, CI-ready, extensible

---

## ðŸ“Š Performance Snapshot

| Metric             | Value            |
|--------------------|------------------|
| Ingestion Rate     | ~100 PDFs/min    |
| p95 Query Latency  | 1.2s             |
| Retrieval Quality  | Recall@10 = 0.84 |
| Cost per Query     | $0.004 (est.)    |

---

## ðŸ§° Tech Stack

| Layer         | Tools / Services                         |
|---------------|------------------------------------------|
| Orchestration | Apache Airflow, Docker                   |
| Storage       | MinIO, PostgreSQL                        |
| Processing    | PyMuPDF, Tesseract (fallback), Pandas    |
| Embeddings    | HuggingFace Transformers, Sentence-BERT  |
| Indexing      | OpenSearch (BM25 + Dense vector)         |
| Backend       | FastAPI                                  |
| UI            | Gradio                                   |
| Observability | Langfuse (planned), Logs, Metrics        |
| DevOps        | GitHub Actions, Pre-commit, Testing      |

---

---

## ðŸ› ï¸ Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/research-ai
cd research-ai
```

### Configure Environment

```bash
cp .env.example .env
# Fill in your MinIO credentials, ports, etc.
```

### Start Services

```bash
docker compose up -d --build
```

---
### Access interfaces:

ðŸŒ Airflow UI: http://localhost:8080

ðŸ§  OpenSearch UI: http://localhost:5601
 (if enabled)

ðŸ“‚ MinIO Console: http://localhost:9001

ðŸ§ª FastAPI Docs: http://localhost:8000/docs

---

## ðŸ§ª Development Roadmap

 M1: Infra foundation (Docker, services, health checks)

 M2: Ingestion DAG + PDF storage

 M3: Text extraction & chunking

 M4: Embedding & Indexing

 M5: Search API + LLM response generation

 M6: Observability, tests, documentation