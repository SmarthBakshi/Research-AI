#!/bin/bash
# Script to update all Python scripts to use environment variables instead of hardcoded credentials

set -e

echo "ðŸ”’ Updating scripts to use environment variables for security..."

# Files to update
FILES=(
    "scripts/process_local_pdfs.py"
    "scripts/process_and_index_gcs.py"
)

# Backup files
echo "ðŸ“¦ Creating backups..."
for file in "${FILES[@]}"; do
    if [ -f "$file" ]; then
        cp "$file" "${file}.bak"
        echo "  âœ“ Backed up $file"
    fi
done

# Update process_local_pdfs.py
echo "ðŸ“ Updating process_local_pdfs.py..."
cat > scripts/process_local_pdfs.py << 'EOFPY'
#!/usr/bin/env python3
"""
Process local PDFs and index them into Cloud SQL + OpenSearch
"""
import os
import sys
import glob
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add services to path
sys.path.insert(0, '/Users/smarthbakshi/Desktop/projects/Research-AI')

from services.processing.extractors.hybrid_extractor import HybridPdfExtractor
from services.processing.normalization.text_normaliser import TextNormalizer
from services.processing.chunking.chunker import Chunker
from services.processing.db_write.db_writer import write_chunks_to_db
from services.embedding.huggingface_embedder import HuggingFaceEmbedder
from services.search.opensearch_store import OpenSearchStore

# Configuration from environment variables
PDF_DIR = "/Users/smarthbakshi/Desktop/projects/Research-AI/data/arxiv"
OPENSEARCH_HOST = os.getenv("GCP_OPENSEARCH_HOST")
OPENSEARCH_PORT = int(os.getenv("GCP_OPENSEARCH_PORT", "9200"))

# Set environment variables for db_writer
os.environ["POSTGRES_HOST"] = os.getenv("GCP_POSTGRES_HOST")
os.environ["POSTGRES_USER"] = os.getenv("GCP_POSTGRES_USER")
os.environ["POSTGRES_PASSWORD"] = os.getenv("GCP_POSTGRES_PASSWORD")
os.environ["APP_DB"] = os.getenv("GCP_POSTGRES_DB")
os.environ["POSTGRES_PORT"] = os.getenv("GCP_POSTGRES_PORT", "5432")

def process_pdf(pdf_path):
    """Process a single PDF: extract, normalize, chunk, save to DB"""
    source_file = os.path.basename(pdf_path)
    print(f"\nðŸ”„ Processing {source_file}...")

    # Initialize components
    extractor = HybridPdfExtractor()
    normalizer = TextNormalizer(remove_latex=True)
    chunker = Chunker()

    # Extract text
    print("  ðŸ“„ Extracting text...")
    result = extractor.extract(pdf_path)

    # Get text from result
    if hasattr(result, "full_text"):
        text = result.full_text
    elif hasattr(result, "text"):
        text = result.text
    else:
        text = str(result) if result else None

    if not text:
        raise ValueError(f"Could not extract text from {source_file}")

    print(f"  âœ“ Extracted {len(text)} characters")

    # Normalize
    print("  ðŸ§¹ Normalizing text...")
    normalized = normalizer.clean(text)

    # Chunk
    print("  âœ‚ï¸  Chunking text...")
    chunks = chunker.chunk(normalized, source_file=source_file)
    print(f"  âœ“ Created {len(chunks)} chunks")

    # Write to database
    print(f"  ðŸ’¾ Writing chunks to Cloud SQL...")
    write_chunks_to_db(chunks)
    print(f"  âœ“ Saved {len(chunks)} chunks to database")

    return chunks, source_file

def index_chunks(source_file):
    """Index chunks from database into OpenSearch"""
    print(f"\nðŸ” Indexing {source_file} into OpenSearch...")

    # Initialize services
    print("  ðŸ¤– Loading embedding model...")
    embedder = HuggingFaceEmbedder(model_name="intfloat/e5-base-v2")

    print("  ðŸ”Œ Connecting to OpenSearch...")
    search_store = OpenSearchStore(
        host=OPENSEARCH_HOST,
        port=OPENSEARCH_PORT,
        index_name="chunks"
    )

    # Get chunks from database
    import psycopg2
    conn = psycopg2.connect(
        host=os.environ["POSTGRES_HOST"],
        user=os.environ["POSTGRES_USER"],
        password=os.environ["POSTGRES_PASSWORD"],
        database=os.environ["APP_DB"],
        port=os.environ["POSTGRES_PORT"]
    )
    cur = conn.cursor()

    cur.execute(
        "SELECT id, source_file, chunk_index, chunk_text FROM chunks WHERE source_file = %s AND indexed_at IS NULL",
        (source_file,)
    )
    rows = cur.fetchall()

    if not rows:
        print(f"  âš ï¸  No unindexed chunks found for {source_file}")
        return

    print(f"  ðŸ“Š Found {len(rows)} chunks to index")

    # Embed and index
    print("  ðŸ§® Generating embeddings...")
    texts = [row[3] for row in rows]
    embeddings = embedder.embed_batch(texts)

    print("  ðŸ“¤ Uploading to OpenSearch...")
    docs = []
    for i, row in enumerate(rows):
        chunk_id, source, chunk_idx, text = row
        docs.append({
            "chunk_text": text,
            "source_file": source,
            "chunk_index": chunk_idx,
            "embedding": embeddings[i].tolist() if hasattr(embeddings[i], 'tolist') else embeddings[i]
        })

    search_store.upsert_documents(docs)

    # Update indexed_at timestamp
    from datetime import datetime
    cur.execute(
        "UPDATE chunks SET indexed_at = %s WHERE source_file = %s AND indexed_at IS NULL",
        (datetime.utcnow(), source_file)
    )
    conn.commit()
    conn.close()

    print(f"  âœ“ Indexed {len(rows)} chunks")

def main():
    """Process all PDFs from local directory"""
    print("ðŸš€ Starting PDF processing and indexing pipeline...")

    # Find all PDFs
    pdf_files = glob.glob(os.path.join(PDF_DIR, "*.pdf"))
    print(f"ðŸ“š Found {len(pdf_files)} PDFs in {PDF_DIR}")

    for pdf_path in pdf_files:
        try:
            # Process PDF
            chunks, source_file = process_pdf(pdf_path)

            # Index chunks
            index_chunks(source_file)

            print(f"âœ… Completed {source_file}\n")

        except Exception as e:
            print(f"âŒ Error processing {os.path.basename(pdf_path)}: {e}\n")
            import traceback
            traceback.print_exc()
            continue

    print("\nðŸŽ‰ All done!")

if __name__ == "__main__":
    main()
EOFPY

# Update process_and_index_gcs.py
echo "ðŸ“ Updating process_and_index_gcs.py..."
cat > scripts/process_and_index_gcs.py << 'EOFPY'
#!/usr/bin/env python3
"""
Process PDFs from GCS and index them into Cloud SQL + OpenSearch
This runs outside of Airflow for quick GCP deployment
"""
import os
import sys
import tempfile
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add services to path
sys.path.insert(0, '/Users/smarthbakshi/Desktop/projects/Research-AI')

from google.cloud import storage
from services.processing.extractors.hybrid_extractor import HybridPdfExtractor
from services.processing.normalization.text_normaliser import TextNormalizer
from services.processing.chunking.chunker import Chunker
from services.processing.db_write.db_writer import write_chunks_to_db
from services.embedding.huggingface_embedder import HuggingFaceEmbedder
from services.search.opensearch_store import OpenSearchStore

# Configuration from environment variables
BUCKET_NAME = "researchai-pdfs-eu"
OPENSEARCH_HOST = os.getenv("GCP_OPENSEARCH_HOST")
OPENSEARCH_PORT = int(os.getenv("GCP_OPENSEARCH_PORT", "9200"))

# Set environment variables for db_writer
os.environ["POSTGRES_HOST"] = os.getenv("GCP_POSTGRES_HOST")
os.environ["POSTGRES_USER"] = os.getenv("GCP_POSTGRES_USER")
os.environ["POSTGRES_PASSWORD"] = os.getenv("GCP_POSTGRES_PASSWORD")
os.environ["APP_DB"] = os.getenv("GCP_POSTGRES_DB")
os.environ["POSTGRES_PORT"] = os.getenv("GCP_POSTGRES_PORT", "5432")

def download_pdf_from_gcs(bucket_name, blob_name):
    """Download PDF from GCS to temporary file"""
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)

    # Create temporary file
    suffix = os.path.splitext(blob_name)[1]
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    temp_path = temp_file.name
    temp_file.close()

    print(f"ðŸ“¥ Downloading {blob_name} from GCS...")
    blob.download_to_filename(temp_path)
    print(f"âœ“ Downloaded to {temp_path}")

    return temp_path

def process_pdf(pdf_path, source_file):
    """Process a single PDF: extract, normalize, chunk, save to DB"""
    print(f"\nðŸ”„ Processing {source_file}...")

    # Initialize components
    extractor = HybridPdfExtractor()
    normalizer = TextNormalizer(remove_latex=True)
    chunker = Chunker()

    # Extract text
    print("  ðŸ“„ Extracting text...")
    result = extractor.extract(pdf_path)

    # Get text from result
    if hasattr(result, "full_text"):
        text = result.full_text
    elif hasattr(result, "text"):
        text = result.text
    else:
        text = str(result) if result else None

    if not text:
        raise ValueError(f"Could not extract text from {source_file}")

    print(f"  âœ“ Extracted {len(text)} characters")

    # Normalize
    print("  ðŸ§¹ Normalizing text...")
    normalized = normalizer.clean(text)

    # Chunk
    print("  âœ‚ï¸  Chunking text...")
    chunks = chunker.chunk(normalized, source_file=source_file)
    print(f"  âœ“ Created {len(chunks)} chunks")

    # Write to database
    print(f"  ðŸ’¾ Writing chunks to Cloud SQL...")
    write_chunks_to_db(chunks)
    print(f"  âœ“ Saved {len(chunks)} chunks to database")

    return chunks

def index_chunks(source_file):
    """Index chunks from database into OpenSearch"""
    print(f"\nðŸ” Indexing {source_file} into OpenSearch...")

    # Initialize services
    embedder = HuggingFaceEmbedder(model_name="intfloat/e5-base-v2")
    search_store = OpenSearchStore(
        host=OPENSEARCH_HOST,
        port=OPENSEARCH_PORT,
        index_name="chunks"
    )

    # Get chunks from database
    import psycopg2
    conn = psycopg2.connect(
        host=os.environ["POSTGRES_HOST"],
        user=os.environ["POSTGRES_USER"],
        password=os.environ["POSTGRES_PASSWORD"],
        database=os.environ["APP_DB"],
        port=os.environ["POSTGRES_PORT"]
    )
    cur = conn.cursor()

    cur.execute(
        "SELECT id, source_file, chunk_index, chunk_text FROM chunks WHERE source_file = %s AND indexed_at IS NULL",
        (source_file,)
    )
    rows = cur.fetchall()

    if not rows:
        print(f"  âš ï¸  No unindexed chunks found for {source_file}")
        return

    print(f"  ðŸ“Š Found {len(rows)} chunks to index")

    # Embed and index
    print("  ðŸ§® Generating embeddings...")
    texts = [row[3] for row in rows]
    embeddings = embedder.embed_batch(texts)

    print("  ðŸ“¤ Uploading to OpenSearch...")
    for i, row in enumerate(rows):
        chunk_id, source, chunk_idx, text = row
        search_store.insert_chunk(
            chunk_id=chunk_id,
            source_file=source,
            chunk_index=chunk_idx,
            chunk_text=text,
            embedding=embeddings[i]
        )

    # Update indexed_at timestamp
    from datetime import datetime
    cur.execute(
        "UPDATE chunks SET indexed_at = %s WHERE source_file = %s AND indexed_at IS NULL",
        (datetime.utcnow(), source_file)
    )
    conn.commit()
    conn.close()

    print(f"  âœ“ Indexed {len(rows)} chunks")

def main():
    """Process all PDFs from GCS"""
    print("ðŸš€ Starting PDF processing and indexing pipeline...")

    # List all PDFs in bucket
    client = storage.Client()
    bucket = client.bucket(BUCKET_NAME)
    blobs = list(bucket.list_blobs())
    pdf_blobs = [b for b in blobs if b.name.endswith('.pdf')]

    print(f"ðŸ“š Found {len(pdf_blobs)} PDFs in gs://{BUCKET_NAME}/")

    for blob in pdf_blobs:
        try:
            # Download PDF
            pdf_path = download_pdf_from_gcs(BUCKET_NAME, blob.name)

            # Process PDF
            chunks = process_pdf(pdf_path, blob.name)

            # Index chunks
            index_chunks(blob.name)

            # Clean up temp file
            os.unlink(pdf_path)

            print(f"âœ… Completed {blob.name}\n")

        except Exception as e:
            print(f"âŒ Error processing {blob.name}: {e}\n")
            import traceback
            traceback.print_exc()
            continue

    print("\nðŸŽ‰ All done!")

if __name__ == "__main__":
    main()
EOFPY

chmod +x scripts/process_local_pdfs.py
chmod +x scripts/process_and_index_gcs.py

echo "âœ… All scripts updated successfully!"
echo ""
echo "âš ï¸  IMPORTANT: Update your .env file with actual credentials:"
echo "   GCP_POSTGRES_PASSWORD=your_actual_password"
echo ""
echo "Backup files created with .bak extension"
